/**
 * æ¨¡å‹è¨“ç·´è…³æœ¬
 * ä½¿ç”¨ TensorFlow.js (ç€è¦½å™¨ç‰ˆæœ¬) å’Œ sharp è¨“ç·´åœ–åƒåˆ†é¡æ¨¡å‹
 * ä¸ä½¿ç”¨ tensorflow-helpersï¼Œå®Œå…¨æ‰‹å‹•å¯¦ç¾
 */

import * as tf from "@tensorflow/tfjs";
import "@tensorflow/tfjs-backend-cpu";
import * as fs from "fs";
import * as path from "path";
import { loadImageAsTensor } from "./image-utils";
import { loadMobileNet, IMAGE_SIZE } from "./model-loader";
import { TrainingLogger } from "./training-logger";

const rootDir = path.resolve(process.cwd());
const baseModelDir = path.join(rootDir, "saved_model/base_model");
const classifierModelDir = path.join(rootDir, "saved_model/classifier_model");
const datasetDir = path.join(rootDir, "dataset");

/**
 * å¾æ•¸æ“šé›†ç›®éŒ„è®€å–æ‰€æœ‰åœ–ç‰‡
 */
function loadDataset(): { [label: string]: string[] } {
  if (!fs.existsSync(datasetDir)) {
    throw new Error(`æ•¸æ“šé›†ç›®éŒ„ä¸å­˜åœ¨: ${datasetDir}`);
  }

  const dataset: { [label: string]: string[] } = {};
  const labelDirs = fs.readdirSync(datasetDir).filter((item) => {
    const itemPath = path.join(datasetDir, item);
    return fs.statSync(itemPath).isDirectory();
  });

  labelDirs.forEach((label) => {
    const labelDir = path.join(datasetDir, label);
    const files = fs.readdirSync(labelDir).filter((file) => {
      const ext = path.extname(file).toLowerCase();
      return [".jpg", ".jpeg", ".png", ".gif", ".webp"].includes(ext);
    });

    dataset[label] = files.map((file) => path.join(labelDir, file));
  });

  return dataset;
}

/**
 * å‰µå»ºåˆ†é¡å™¨æ¨¡å‹ï¼ˆåœ¨ MobileNet åŸºç¤ä¸Šæ·»åŠ åˆ†é¡å±¤ï¼‰
 */
function createClassifier(
  baseModel: tf.LayersModel,
  numClasses: number,
  hiddenUnits: number = 128
): tf.LayersModel {
  console.log(`ğŸ”§ å‰µå»ºåˆ†é¡å™¨ (${numClasses} å€‹é¡åˆ¥)...`);

  // ç²å–åŸºç¤æ¨¡å‹çš„è¼¸å‡ºï¼ˆç‰¹å¾µæå–éƒ¨åˆ†ï¼‰
  // model.output å¯èƒ½æ˜¯ SymbolicTensor æˆ– SymbolicTensor[]
  const baseOutput = Array.isArray(baseModel.output)
    ? baseModel.output[0]
    : baseModel.output;

  // æª¢æŸ¥è¼¸å‡ºå½¢ç‹€ï¼Œæ±ºå®šè™•ç†æ–¹å¼
  const outputShape = baseOutput.shape;
  console.log(`   åŸºç¤æ¨¡å‹è¼¸å‡ºå½¢ç‹€: ${JSON.stringify(outputShape)}`);

  let features: tf.SymbolicTensor = baseOutput;

  // å¦‚æœè¼¸å‡ºæ˜¯ 2D ä¸”åªæœ‰ 1000 å€‹å–®å…ƒï¼ˆMobileNet å®Œæ•´åˆ†é¡è¼¸å‡ºï¼‰
  if (outputShape && outputShape.length === 2 && outputShape[1] === 1000) {
    // MobileNet è¼¸å‡ºæ˜¯å®Œæ•´åˆ†é¡çµæœï¼Œæˆ‘å€‘éœ€è¦å¾ä¸­é–“å±¤æå–ç‰¹å¾µ
    // å˜—è©¦æ‰¾åˆ°åˆ†é¡å±¤ä¹‹å‰çš„ç‰¹å¾µæå–å±¤
    const layers = baseModel.layers;
    let featureLayer: tf.layers.Layer | null = null;

    // å¾å¾Œå¾€å‰æ‰¾ï¼Œè·³éåˆ†é¡ç›¸é—œå±¤ï¼ˆdropout, conv_preds, act_softmax, reshapeï¼‰
    for (let i = layers.length - 1; i >= 0; i--) {
      const layer = layers[i];
      const layerName = layer.name.toLowerCase();
      // å°‹æ‰¾å…¨å±€æ± åŒ–å±¤æˆ–é‡å¡‘å±¤ä¹‹å‰çš„å±¤
      if (
        layerName.includes("global_average_pooling") ||
        (layerName.includes("reshape") && !layerName.includes("2"))
      ) {
        featureLayer = layer;
        break;
      }
    }

    if (featureLayer) {
      // ç›´æ¥ä½¿ç”¨ç‰¹å¾µå±¤çš„è¼¸å‡ºï¼Œä¸å‰µå»ºè‡¨æ™‚æ¨¡å‹ï¼ˆé¿å… dispose å•é¡Œï¼‰
      let extractedFeatures = Array.isArray(featureLayer.output)
        ? featureLayer.output[0]
        : featureLayer.output;

      console.log(`   âœ… å¾å±¤ "${featureLayer.name}" æå–ç‰¹å¾µ`);
      console.log(`   ç‰¹å¾µå½¢ç‹€: ${JSON.stringify(extractedFeatures.shape)}`);

      // å¦‚æœç‰¹å¾µæ˜¯ 4Dï¼Œéœ€è¦ flatten æˆ– global pooling
      if (extractedFeatures.shape && extractedFeatures.shape.length === 4) {
        features = tf.layers
          .globalAveragePooling2d({
            name: "classifier_feature_pool",
          })
          .apply(extractedFeatures) as tf.SymbolicTensor;
      } else if (
        extractedFeatures.shape &&
        extractedFeatures.shape.length > 2
      ) {
        // å¦‚æœæ˜¯ 3D æˆ–å…¶ä»–å¤šç¶­ï¼Œä½¿ç”¨ flatten
        features = tf.layers
          .flatten({
            name: "classifier_feature_flatten",
          })
          .apply(extractedFeatures) as tf.SymbolicTensor;
      } else {
        features = extractedFeatures;
      }
    } else {
      // å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æŠ•å½±å±¤å°‡ 1000 ç¶­æ˜ å°„åˆ°ç‰¹å¾µç¶­åº¦
      console.log("   âš ï¸  æœªæ‰¾åˆ°ç‰¹å¾µå±¤ï¼Œä½¿ç”¨æŠ•å½±å±¤æ˜ å°„");
      features = tf.layers
        .dense({
          units: 512,
          activation: "relu",
          name: "feature_projection",
          useBias: true,
        })
        .apply(baseOutput) as tf.SymbolicTensor;
    }
  } else if (outputShape && outputShape.length === 4) {
    // å¦‚æœè¼¸å‡ºæ˜¯ 4Dï¼ˆåŒ…å«ç©ºé–“ç¶­åº¦ï¼‰ï¼Œéœ€è¦å…¨å±€å¹³å‡æ± åŒ–
    features = tf.layers
      .globalAveragePooling2d({
        name: "classifier_global_avg_pool",
      })
      .apply(baseOutput) as tf.SymbolicTensor;
  }

  // æ·»åŠ éš±è—å±¤
  const hidden = tf.layers
    .dense({
      units: hiddenUnits,
      activation: "relu",
      name: "classifier_hidden",
    })
    .apply(features) as tf.SymbolicTensor;

  // æ·»åŠ  Dropout é˜²æ­¢éæ“¬åˆï¼ˆä½¿ç”¨å”¯ä¸€åç¨±é¿å…è¡çªï¼‰
  const dropout = tf.layers
    .dropout({
      rate: 0.5,
      name: "classifier_dropout",
    })
    .apply(hidden) as tf.SymbolicTensor;

  // æ·»åŠ åˆ†é¡å±¤
  const output = tf.layers
    .dense({
      units: numClasses,
      activation: "softmax",
      name: "classifier_output",
    })
    .apply(dropout) as tf.SymbolicTensor;

  // å‰µå»ºæ–°æ¨¡å‹
  // model.input ä¹Ÿå¯èƒ½æ˜¯ SymbolicTensor æˆ– SymbolicTensor[]
  const baseInput = Array.isArray(baseModel.input)
    ? baseModel.input[0]
    : baseModel.input;

  const classifier = tf.model({
    inputs: baseInput,
    outputs: output,
  });

  // å‡çµåŸºç¤æ¨¡å‹çš„å±¤ï¼ˆåªè¨“ç·´åˆ†é¡å±¤ï¼‰
  // æ³¨æ„ï¼šæˆ‘å€‘åªå‡çµåŸºç¤æ¨¡å‹çš„å‰ N-1 å±¤ï¼Œæœ€å¾Œçš„ç‰¹å¾µå±¤ä¸å‡çµï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
  const numBaseLayers = baseModel.layers.length;
  for (let i = 0; i < numBaseLayers; i++) {
    const layer = baseModel.layers[i];
    // åªå‡çµåŸºç¤æ¨¡å‹çš„å±¤ï¼Œä¸åŒ…æ‹¬æˆ‘å€‘æ–°åŠ çš„å±¤
    if (classifier.layers.some((l) => l === layer)) {
      layer.trainable = false;
    }
  }

  // å‡çµæ‰€æœ‰åŸºç¤æ¨¡å‹çš„å±¤ï¼ˆæ›´å®‰å…¨çš„æ–¹æ³•ï¼‰
  baseModel.layers.forEach((layer) => {
    layer.trainable = false;
  });

  console.log("âœ… åˆ†é¡å™¨å‰µå»ºå®Œæˆ\n");

  return classifier;
}

/**
 * æº–å‚™è¨“ç·´æ•¸æ“š
 */
async function prepareTrainingData(
  dataset: { [label: string]: string[] },
  classNames: string[]
): Promise<{
  xs: tf.Tensor4D;
  ys: tf.Tensor2D;
}> {
  console.log("ğŸ“Š æº–å‚™è¨“ç·´æ•¸æ“š...");

  const allImages: string[] = [];
  const allLabels: number[] = [];

  // æ”¶é›†æ‰€æœ‰åœ–ç‰‡å’Œå°æ‡‰çš„æ¨™ç±¤
  classNames.forEach((className, classIndex) => {
    const images = dataset[className] || [];
    images.forEach((imagePath) => {
      allImages.push(imagePath);
      allLabels.push(classIndex);
    });
  });

  if (allImages.length === 0) {
    throw new Error("æ²’æœ‰æ‰¾åˆ°è¨“ç·´åœ–ç‰‡");
  }

  console.log(
    `   ç¸½å…± ${allImages.length} å¼µåœ–ç‰‡ï¼Œ${classNames.length} å€‹é¡åˆ¥`
  );

  // è¼‰å…¥æ‰€æœ‰åœ–ç‰‡
  const tensors: tf.Tensor4D[] = [];
  let loaded = 0;

  for (const imagePath of allImages) {
    try {
      const tensor = await loadImageAsTensor(imagePath, IMAGE_SIZE);
      tensors.push(tensor);
      loaded++;

      if (loaded % 10 === 0) {
        process.stdout.write(`   å·²è¼‰å…¥: ${loaded}/${allImages.length}\r`);
      }
    } catch (error) {
      console.warn(`\n   è·³éåœ–ç‰‡: ${imagePath}`);
    }
  }

  console.log(`\n   âœ… æˆåŠŸè¼‰å…¥ ${loaded} å¼µåœ–ç‰‡\n`);

  // åˆä½µç‚ºæ‰¹æ¬¡
  const xs = tf.concat(tensors, 0) as tf.Tensor4D;

  // å‰µå»ºæ¨™ç±¤ï¼ˆone-hot encodingï¼‰
  const ys = tf.oneHot(
    tf.tensor1d(allLabels, "int32"),
    classNames.length
  ) as tf.Tensor2D;

  // æ¸…ç†ä¸­é–“ tensor
  tensors.forEach((t) => t.dispose());

  return { xs, ys };
}

/**
 * è¨“ç·´æ¨¡å‹
 */
async function trainModel(
  model: tf.LayersModel,
  xs: tf.Tensor4D,
  ys: tf.Tensor2D,
  epochs: number = 10,
  batchSize: number = 32,
  logger?: TrainingLogger
): Promise<any> {
  console.log("ğŸ¯ é–‹å§‹è¨“ç·´æ¨¡å‹...");
  console.log(`   è¨“ç·´è¼ªæ•¸: ${epochs}`);
  console.log(`   æ‰¹æ¬¡å¤§å°: ${batchSize}\n`);

  // ç·¨è­¯æ¨¡å‹
  const learningRate = 0.001;
  model.compile({
    optimizer: tf.train.adam(learningRate),
    loss: "categoricalCrossentropy",
    metrics: ["accuracy"],
  });

  // è¨“ç·´æ¨¡å‹
  const history = await model.fit(xs, ys, {
    epochs,
    batchSize,
    shuffle: true,
    validationSplit: 0.2, // 20% ç”¨æ–¼é©—è­‰
    callbacks: {
      onEpochEnd: (epoch: number, logs?: any) => {
        const loss = logs?.loss ? Number(logs.loss).toFixed(4) : "N/A";
        const acc = logs?.acc ? Number(logs.acc).toFixed(4) : "N/A";
        const valLoss = logs?.val_loss
          ? Number(logs.val_loss).toFixed(4)
          : "N/A";
        const valAcc = logs?.val_acc ? Number(logs.val_acc).toFixed(4) : "N/A";
        console.log(
          `   Epoch ${epoch + 1}/${epochs} - ` +
            `loss: ${loss} - ` +
            `acc: ${acc} - ` +
            `val_loss: ${valLoss} - ` +
            `val_acc: ${valAcc}`
        );

        // è¨˜éŒ„åˆ°è¨“ç·´æ—¥èªŒ
        if (logger) {
          logger.logEpoch(epoch, logs);
        }
      },
    },
  });

  console.log("\nâœ… è¨“ç·´å®Œæˆï¼\n");

  return history;
}

/**
 * ä¿å­˜æ¨¡å‹
 * ç”±æ–¼ TensorFlow.js ç€è¦½å™¨ç‰ˆæœ¬ä¸æ”¯æŒ file:// å”è­°ï¼Œæˆ‘å€‘æ‰‹å‹•ä¿å­˜æ¨¡å‹
 */
async function saveModel(
  model: tf.LayersModel,
  modelDir: string
): Promise<void> {
  console.log("ğŸ’¾ ä¿å­˜æ¨¡å‹...");

  // ç¢ºä¿ç›®éŒ„å­˜åœ¨
  if (!fs.existsSync(modelDir)) {
    fs.mkdirSync(modelDir, { recursive: true });
  }

  try {
    // å˜—è©¦ä½¿ç”¨æ¨™æº–æ–¹å¼ä¿å­˜ï¼ˆå¦‚æœæ”¯æŒï¼‰
    await model.save(`file://${modelDir}`);
    console.log(`âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: ${modelDir}\n`);
  } catch (error) {
    // å¦‚æœå¤±æ•—ï¼Œä½¿ç”¨æ‰‹å‹•ä¿å­˜æ–¹å¼
    console.log("âš ï¸  æ¨™æº–ä¿å­˜æ–¹å¼å¤±æ•—ï¼Œä½¿ç”¨æ‰‹å‹•ä¿å­˜æ–¹å¼...");
    await saveModelManually(model, modelDir);
  }
}

/**
 * æ‰‹å‹•ä¿å­˜æ¨¡å‹ï¼ˆé©ç”¨æ–¼ç€è¦½å™¨ç‰ˆæœ¬çš„ TensorFlow.jsï¼‰
 */
async function saveModelManually(
  model: tf.LayersModel,
  modelDir: string
): Promise<void> {
  // 1. ä¿å­˜æ¨¡å‹çµæ§‹ï¼ˆJSONï¼‰
  const modelJson = model.toJSON();
  const modelJsonPath = path.join(modelDir, "model.json");
  fs.writeFileSync(modelJsonPath, JSON.stringify(modelJson, null, 2), "utf-8");
  console.log(`   âœ… æ¨¡å‹çµæ§‹å·²ä¿å­˜: ${path.basename(modelJsonPath)}`);

  // 2. æ”¶é›†æ‰€æœ‰æ¬Šé‡ä¸¦ä¿å­˜
  const weightManifest: Array<{
    name: string;
    shape: (number | null)[];
    dtype: string;
  }> = [];

  let totalWeights = 0;
  for (const weight of model.weights) {
    // ä½¿ç”¨ read() æ–¹æ³•ç²å–æ¬Šé‡å€¼ï¼ˆé¿å…ç›´æ¥è¨ªå•å—ä¿è­·çš„ val å±¬æ€§ï¼‰
    const weightTensor = weight.read();
    const values = await weightTensor.array();
    const flattened = (values as any).flat(Infinity) as number[];

    // ä¿å­˜æ¯å€‹æ¬Šé‡ç‚ºå–®ç¨çš„æ–‡ä»¶
    const weightName = weight.name.replace(/\//g, "_").replace(/:/g, "_");
    const weightPath = path.join(modelDir, `${weightName}.bin`);
    const buffer = Buffer.from(new Float32Array(flattened).buffer);
    fs.writeFileSync(weightPath, buffer);

    weightManifest.push({
      name: weight.name,
      shape: weight.shape, // Shape é¡å‹å¯èƒ½åŒ…å« nullï¼Œé€™è£¡ä¿ç•™åŸå§‹é¡å‹
      dtype: weight.dtype,
    });

    totalWeights += flattened.length;

    // æ¸…ç†è‡¨æ™‚ tensor
    weightTensor.dispose();
  }

  // 3. ä¿å­˜æ¬Šé‡æ¸…å–®
  const manifestPath = path.join(modelDir, "weights-manifest.json");
  fs.writeFileSync(
    manifestPath,
    JSON.stringify(weightManifest, null, 2),
    "utf-8"
  );
  console.log(`   âœ… æ¬Šé‡æ¸…å–®å·²ä¿å­˜: ${path.basename(manifestPath)}`);
  console.log(`   âœ… ç¸½å…±ä¿å­˜ ${weightManifest.length} å€‹æ¬Šé‡å¼µé‡`);

  console.log(`\nâœ… æ¨¡å‹å·²æˆåŠŸä¿å­˜åˆ°: ${modelDir}`);
}

/**
 * å˜—è©¦è¼‰å…¥å·²è¨“ç·´çš„æ¨¡å‹
 */
async function tryLoadExistingModel(
  modelDir: string
): Promise<tf.LayersModel | null> {
  const modelJsonPath = path.join(modelDir, "model.json");

  if (!fs.existsSync(modelJsonPath)) {
    console.log("   â„¹ï¸  æœªæ‰¾åˆ°å·²æœ‰æ¨¡å‹æ–‡ä»¶ï¼ˆmodel.json ä¸å­˜åœ¨ï¼‰");
    return null;
  }

  try {
    console.log("ğŸ“¦ æª¢æ¸¬åˆ°å·²æœ‰æ¨¡å‹ï¼Œå˜—è©¦è¼‰å…¥...");
    console.log(`   æ¨¡å‹è·¯å¾‘: ${modelJsonPath}`);

    // å˜—è©¦è¼‰å…¥æ¨¡å‹ï¼ˆTensorFlow.js æœƒè‡ªå‹•è™•ç†æ¬Šé‡æ–‡ä»¶ï¼‰
    const model = await tf.loadLayersModel(`file://${modelJsonPath}`);
    console.log("   âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ");
    return model;
  } catch (error: any) {
    console.log("   âš ï¸  æ¨¡å‹è¼‰å…¥å¤±æ•—:", error.message || error);
    console.log("   ğŸ’¡ æç¤ºï¼šé€™å¯èƒ½æ˜¯å› ç‚ºæ¨¡å‹ä¿å­˜æ ¼å¼ä¸å…¼å®¹ï¼Œå°‡ä½¿ç”¨å…¨æ–°è¨“ç·´");
    return null;
  }
}

/**
 * ä¸»è¨“ç·´å‡½æ•¸
 */
async function train(continueTraining: boolean = false) {
  const epochs = 5; // è¨“ç·´è¼ªæ•¸ï¼š15 å€‹ epochs
  const batchSize = 32;
  const learningRate = 0.001;
  const validationSplit = 0.2;
  let logger: TrainingLogger | undefined;

  try {
    console.log("ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹...\n");

    // 1. è¼‰å…¥æ•¸æ“šé›†
    console.log("ğŸ“‚ è®€å–æ•¸æ“šé›†...");
    const dataset = loadDataset();
    const classNames = Object.keys(dataset).sort();

    if (classNames.length === 0) {
      throw new Error("æ•¸æ“šé›†ç‚ºç©ºï¼Œè«‹å…ˆæº–å‚™è¨“ç·´æ•¸æ“š");
    }

    const totalImages = Object.values(dataset).reduce(
      (sum, images) => sum + images.length,
      0
    );
    console.log(
      `âœ… æ‰¾åˆ° ${classNames.length} å€‹é¡åˆ¥ï¼Œå…± ${totalImages} å¼µåœ–ç‰‡`
    );
    console.log(`   é¡åˆ¥: ${classNames.join(", ")}\n`);

    // åˆå§‹åŒ–è¨“ç·´è¨˜éŒ„å™¨
    const logDir = path.join(rootDir, "training_logs");
    logger = new TrainingLogger(logDir, {
      totalEpochs: epochs,
      batchSize,
      learningRate,
      optimizer: "adam",
      lossFunction: "categoricalCrossentropy",
      validationSplit,
      imageSize: IMAGE_SIZE,
      numClasses: classNames.length,
      totalImages,
      classNames,
      baseModel: "MobileNet",
      featureExtractor: "unknown",
    });

    // 2. è¼‰å…¥æˆ–å‰µå»ºåˆ†é¡å™¨
    let classifier: tf.LayersModel;

    if (continueTraining) {
      // å˜—è©¦è¼‰å…¥å·²æœ‰æ¨¡å‹
      const existingModel = await tryLoadExistingModel(classifierModelDir);
      if (existingModel) {
        console.log("âœ… æˆåŠŸè¼‰å…¥å·²æœ‰æ¨¡å‹ï¼Œå°‡ç¹¼çºŒè¨“ç·´\n");
        classifier = existingModel;
      } else {
        console.log("âš ï¸  ç„¡æ³•è¼‰å…¥å·²æœ‰æ¨¡å‹ï¼Œå°‡å‰µå»ºæ–°æ¨¡å‹\n");
        // è¼‰å…¥åŸºç¤æ¨¡å‹ä¸¦å‰µå»ºæ–°åˆ†é¡å™¨
        const baseModel = await loadMobileNet(baseModelDir);
        classifier = createClassifier(baseModel, classNames.length);

        const baseModelName = baseModel.name || "MobileNet";
        if (logger) {
          logger.updateMetadata({
            baseModel: baseModelName,
            featureExtractor: "reshape_1 (1024 dim)",
          });
        }
      }
    } else {
      // å…¨æ–°è¨“ç·´ï¼šè¼‰å…¥åŸºç¤æ¨¡å‹ä¸¦å‰µå»ºæ–°åˆ†é¡å™¨
      const baseModel = await loadMobileNet(baseModelDir);

      // æ›´æ–°è¨˜éŒ„å™¨çš„åŸºç¤æ¨¡å‹ä¿¡æ¯
      const baseModelName = baseModel.name || "MobileNet";
      logger.updateMetadata({
        baseModel: baseModelName,
        featureExtractor: "reshape_1 (1024 dim)",
      });

      classifier = createClassifier(baseModel, classNames.length);
    }

    // 4. æº–å‚™è¨“ç·´æ•¸æ“š
    const { xs, ys } = await prepareTrainingData(dataset, classNames);

    // 5. è¨“ç·´æ¨¡å‹
    const history = await trainModel(
      classifier,
      xs,
      ys,
      epochs,
      batchSize,
      logger
    );

    // 6. æ¸…ç†
    xs.dispose();
    ys.dispose();

    // å®Œæˆè¨“ç·´è¨˜éŒ„
    if (logger) {
      logger.finish();
      console.log(logger.generateSummary());
    }

    // 7. ä¿å­˜æ¨¡å‹
    await saveModel(classifier, classifierModelDir);

    // 8. ä¿å­˜é¡åˆ¥åç¨±ï¼ˆç”¨æ–¼æ¨ç†æ™‚ä½¿ç”¨ï¼‰
    const classNamesPath = path.join(classifierModelDir, "classNames.json");
    fs.writeFileSync(classNamesPath, JSON.stringify(classNames, null, 2));

    console.log("ğŸ‰ è¨“ç·´æµç¨‹å®Œæˆï¼");
    console.log(`\næ¨¡å‹å·²ä¿å­˜ï¼Œé¡åˆ¥ä¿¡æ¯å·²ä¿å­˜åˆ°: ${classNamesPath}`);

    if (logger) {
      console.log(`\nğŸ“Š è¨“ç·´è¨˜éŒ„å·²ä¿å­˜åˆ°: ${logger.getLogFilePath()}`);
    }
  } catch (error) {
    // å³ä½¿å¤±æ•—ä¹Ÿè¨˜éŒ„
    if (logger) {
      logger.finish();
    }
    console.error("âŒ è¨“ç·´å¤±æ•—:", error);
    throw error;
  }
}

// å¦‚æœç›´æ¥é‹è¡Œæ­¤æ–‡ä»¶
if (require.main === module) {
  // æª¢æŸ¥å‘½ä»¤è¡Œåƒæ•¸
  const continueTraining =
    process.argv.includes("--continue") ||
    process.argv.includes("-c") ||
    process.argv[2] === "continue";

  if (continueTraining) {
    console.log("ğŸ”„ ç¹¼çºŒè¨“ç·´æ¨¡å¼ï¼šå°‡å˜—è©¦è¼‰å…¥å·²æœ‰æ¨¡å‹\n");
  } else {
    console.log("ğŸ†• å…¨æ–°è¨“ç·´æ¨¡å¼ï¼šå°‡å‰µå»ºæ–°æ¨¡å‹\n");
  }

  train(continueTraining)
    .then(() => {
      console.log("\nâœ… æ‰€æœ‰æ“ä½œå®Œæˆ");
      process.exit(0);
    })
    .catch((error) => {
      console.error("\nâŒ ç™¼ç”ŸéŒ¯èª¤:", error);
      process.exit(1);
    });
}

export { train };
