/**
 * åœ–åƒåˆ†é¡å™¨æ¨¡çµ„
 * è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹ä¸¦é€²è¡Œåœ–ç‰‡åˆ†é¡é æ¸¬
 */

import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-backend-cpu';
import * as path from 'path';
import * as fs from 'fs';
import { loadImageAsTensor } from './image-utils';
import { IMAGE_SIZE } from './model-loader';

const rootDir = path.resolve(process.cwd());
const classifierModelDir = path.join(rootDir, 'saved_model/classifier_model');

// ç·©å­˜çš„æ¨¡å‹å’Œé¡åˆ¥åç¨±
let cachedModel: tf.LayersModel | null = null;
let cachedClassNames: string[] | null = null;

/**
 * è¼‰å…¥è¨“ç·´å¥½çš„åˆ†é¡å™¨æ¨¡å‹
 */
async function loadClassifierModel(): Promise<tf.LayersModel> {
  if (cachedModel) {
    return cachedModel;
  }

  console.log('ğŸ“¦ æ­£åœ¨è¼‰å…¥åˆ†é¡å™¨æ¨¡å‹...');

  const modelJsonPath = path.join(classifierModelDir, 'model.json');
  const weightsManifestPath = path.join(classifierModelDir, 'weights-manifest.json');

  if (!fs.existsSync(modelJsonPath)) {
    throw new Error(`æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: ${modelJsonPath}`);
  }

  if (!fs.existsSync(weightsManifestPath)) {
    throw new Error(`æ¬Šé‡æ¸…å–®æ–‡ä»¶ä¸å­˜åœ¨: ${weightsManifestPath}`);
  }

  try {
    // é¦–å…ˆå˜—è©¦ä½¿ç”¨æ¨™æº–æ–¹å¼è¼‰å…¥ï¼ˆfile:// å”è­°ï¼‰
    try {
      const model = await tf.loadLayersModel(`file://${modelJsonPath}`);
      console.log('âœ… ä½¿ç”¨æ¨™æº–æ–¹å¼è¼‰å…¥åˆ†é¡å™¨æ¨¡å‹');
      cachedModel = model;
      return model;
    } catch (standardError: any) {
      // å¦‚æœæ¨™æº–æ–¹å¼å¤±æ•—ï¼Œä½¿ç”¨æ‰‹å‹•è¼‰å…¥æ–¹å¼
      console.log('âš ï¸  æ¨™æº–è¼‰å…¥æ–¹å¼å¤±æ•—ï¼Œå˜—è©¦æ‰‹å‹•è¼‰å…¥æ–¹å¼...');
      console.log(`   éŒ¯èª¤: ${standardError.message || standardError}`);
      
      // ä½¿ç”¨æ‰‹å‹•è¼‰å…¥æ–¹å¼
      return await loadModelManually();
    }
  } catch (error) {
    console.error('âŒ è¼‰å…¥åˆ†é¡å™¨æ¨¡å‹å¤±æ•—:', error);
    throw error;
  }
}

/**
 * æ‰‹å‹•è¼‰å…¥æ¨¡å‹ï¼ˆå¾æ‰‹å‹•ä¿å­˜çš„æ–‡ä»¶ï¼‰
 */
async function loadModelManually(): Promise<tf.LayersModel> {
  const modelJsonPath = path.join(classifierModelDir, 'model.json');
  const weightsManifestPath = path.join(classifierModelDir, 'weights-manifest.json');

  try {
    console.log('   é–‹å§‹æ‰‹å‹•è¼‰å…¥æ¨¡å‹çµæ§‹å’Œæ¬Šé‡...');
    
    // 1. è¼‰å…¥æ¨¡å‹çµæ§‹
    // æ³¨æ„ï¼šmodel.json å¯èƒ½æ˜¯ä¸€å€‹å­—ç¬¦ä¸²åŒ–çš„ JSONï¼Œéœ€è¦è§£æå…©æ¬¡
    const modelJsonContent = fs.readFileSync(modelJsonPath, 'utf-8');
    let modelJson: any;
    try {
      // å…ˆè§£æä¸€æ¬¡
      modelJson = JSON.parse(modelJsonContent);
      // å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå†è§£æä¸€æ¬¡
      if (typeof modelJson === 'string') {
        modelJson = JSON.parse(modelJson);
      }
    } catch (error) {
      // å¦‚æœè§£æå¤±æ•—ï¼Œå¯èƒ½æ˜¯æ ¼å¼éŒ¯èª¤
      throw new Error(`ç„¡æ³•è§£ææ¨¡å‹ JSON æ–‡ä»¶: ${error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'}`);
    }
    
    // 2. è¼‰å…¥æ¬Šé‡æ¸…å–®
    const weightManifest: Array<{
      name: string;
      shape: (number | null)[];
      dtype: string;
    }> = JSON.parse(fs.readFileSync(weightsManifestPath, 'utf-8'));
    
    console.log(`   ğŸ“¦ é–‹å§‹è¼‰å…¥ ${weightManifest.length} å€‹æ¬Šé‡...`);

    // 3. è¼‰å…¥æ‰€æœ‰æ¬Šé‡
    const weightTensors: tf.Tensor[] = [];
    for (const item of weightManifest) {
      const weightName = item.name.replace(/\//g, '_').replace(/:/g, '_');
      const weightPath = path.join(classifierModelDir, `${weightName}.bin`);
      
      if (!fs.existsSync(weightPath)) {
        throw new Error(`æ¬Šé‡æ–‡ä»¶ä¸å­˜åœ¨: ${weightPath}`);
      }

      const buffer = fs.readFileSync(weightPath);
      const values = new Float32Array(
        buffer.buffer,
        buffer.byteOffset,
        buffer.byteLength / Float32Array.BYTES_PER_ELEMENT
      );
      
      const validShape = item.shape.filter((s): s is number => s !== null) as number[];
      const tensor = tf.tensor(values, validShape, item.dtype as tf.DataType);
      weightTensors.push(tensor);
    }
    
    console.log('   âœ… æ‰€æœ‰æ¬Šé‡è¼‰å…¥æˆåŠŸ');

    // 4. æ§‹å»ºå®Œæ•´çš„æ¬Šé‡æ•¸æ“šç·©è¡å€
    // TensorFlow.js éœ€è¦å®Œæ•´çš„æ¬Šé‡æ•¸æ“šï¼Œä¸èƒ½ä½¿ç”¨ç©ºç·©è¡å€
    console.log('   ğŸ”§ æ§‹å»ºæ¬Šé‡æ•¸æ“šç·©è¡å€...');
    let totalSize = 0;
    const weightOffsets: number[] = [];
    
    // è¨ˆç®—ç¸½å¤§å°å’Œæ¯å€‹æ¬Šé‡çš„åç§»é‡
    for (let i = 0; i < weightTensors.length; i++) {
      weightOffsets.push(totalSize);
      const shape = weightTensors[i].shape;
      const size = shape.reduce((a, b) => a * b, 1) * 4; // Float32 = 4 bytes
      totalSize += size;
    }
    
    // å‰µå»ºå®Œæ•´çš„æ¬Šé‡æ•¸æ“šç·©è¡å€
    const weightDataBuffer = new ArrayBuffer(totalSize);
    const weightDataView = new Float32Array(weightDataBuffer);
    
    // å°‡æ‰€æœ‰æ¬Šé‡æ•¸æ“šè¤‡è£½åˆ°ç·©è¡å€
    for (let i = 0; i < weightTensors.length; i++) {
      const tensor = weightTensors[i];
      const values = await tensor.array();
      const flattened = (values as any).flat(Infinity) as number[];
      weightDataView.set(flattened, weightOffsets[i] / 4);
    }
    
    console.log('   âœ… æ¬Šé‡æ•¸æ“šç·©è¡å€æ§‹å»ºå®Œæˆ');

    // 5. å¾ JSON å‰µå»ºæ¨¡å‹çµæ§‹
    // æ–¹æ³•ï¼šä½¿ç”¨ tf.loadLayersModel é…åˆåŒ…å«å®Œæ•´æ¬Šé‡æ•¸æ“šçš„ IO handler
    let model: tf.LayersModel;
    try {
      // å‰µå»ºæ¬Šé‡è¦æ ¼
      // WeightsManifestEntry çš„ dtype éœ€è¦æ˜¯ç‰¹å®šçš„å­—é¢é‡é¡å‹ï¼Œè€Œä¸æ˜¯é€šç”¨ string
      type DataType = "string" | "float32" | "int32" | "bool" | "complex64";
      const weightSpecs: tf.io.WeightsManifestEntry[] = weightManifest.map(item => {
        const entry: tf.io.WeightsManifestEntry = {
          name: item.name,
          shape: item.shape.filter((s): s is number => s !== null) as number[],
          dtype: item.dtype as DataType,
        };
        return entry;
      });
      
      // å‰µå»ºä¸€å€‹è‡ªå®šç¾© IO handlerï¼Œå¾å…§å­˜è¼‰å…¥æ¨¡å‹çµæ§‹å’Œæ¬Šé‡
      const customIOHandler: tf.io.IOHandler = {
        load: async () => {
          return {
            modelTopology: modelJson,
            weightSpecs: weightSpecs,
            weightData: weightDataBuffer, // ä½¿ç”¨å®Œæ•´çš„æ¬Šé‡æ•¸æ“šç·©è¡å€
          };
        },
      };
      
      // ä½¿ç”¨è‡ªå®šç¾© IO handler è¼‰å…¥æ¨¡å‹ï¼ˆåŒ…å«çµæ§‹å’Œæ¬Šé‡ï¼‰
      model = await tf.loadLayersModel(customIOHandler);
      console.log('   âœ… ä½¿ç”¨è‡ªå®šç¾© IO handler è¼‰å…¥æ¨¡å‹æˆåŠŸï¼ˆåŒ…å«çµæ§‹å’Œæ¬Šé‡ï¼‰');
      
      // æ¸…ç†è‡¨æ™‚æ¬Šé‡ tensorï¼ˆæ¨¡å‹ç¾åœ¨ä½¿ç”¨ç·©è¡å€ä¸­çš„æ•¸æ“šï¼‰
      weightTensors.forEach(t => t.dispose());
    } catch (error: any) {
      // å¦‚æœè¼‰å…¥å¤±æ•—ï¼Œæ¸…ç†æ¬Šé‡ tensor
      weightTensors.forEach(t => t.dispose());
      console.error('   âŒ å‰µå»ºæ¨¡å‹å¤±æ•—:', error.message || error);
      throw new Error(`ç„¡æ³•å¾ JSON å‰µå»ºæ¨¡å‹çµæ§‹: ${error.message || error}`);
    }

    console.log('âœ… ä½¿ç”¨æ‰‹å‹•è¼‰å…¥æ–¹å¼è¼‰å…¥åˆ†é¡å™¨æ¨¡å‹æˆåŠŸ');
    cachedModel = model;
    return model;
  } catch (error: any) {
    console.error('âŒ æ‰‹å‹•è¼‰å…¥æ¨¡å‹å¤±æ•—:', error);
    throw new Error(
      `æ¨¡å‹è¼‰å…¥å¤±æ•—ã€‚è«‹ç¢ºä¿æ‰€æœ‰æ¬Šé‡æ–‡ä»¶éƒ½å­˜åœ¨æ–¼ ${classifierModelDir} ç›®éŒ„ä¸­ã€‚\n` +
      `éŒ¯èª¤è©³æƒ…: ${error.message || error}`
    );
  }
}

/**
 * è¼‰å…¥é¡åˆ¥åç¨±åˆ—è¡¨
 */
function loadClassNames(): string[] {
  if (cachedClassNames) {
    return cachedClassNames;
  }

  const classNamesPath = path.join(classifierModelDir, 'classNames.json');
  
  if (!fs.existsSync(classNamesPath)) {
    throw new Error(`é¡åˆ¥åç¨±æ–‡ä»¶ä¸å­˜åœ¨: ${classNamesPath}`);
  }

  try {
    const classNames = JSON.parse(fs.readFileSync(classNamesPath, 'utf-8'));
    cachedClassNames = classNames;
    console.log(`âœ… è¼‰å…¥ ${classNames.length} å€‹é¡åˆ¥åç¨±`);
    return classNames;
  } catch (error) {
    console.error('âŒ è¼‰å…¥é¡åˆ¥åç¨±å¤±æ•—:', error);
    throw error;
  }
}

/**
 * å°å–®å¼µåœ–ç‰‡é€²è¡Œåˆ†é¡é æ¸¬
 * @param imagePath åœ–ç‰‡è·¯å¾‘
 * @param topK è¿”å›å‰ K å€‹é æ¸¬çµæœï¼ˆé»˜èª 3ï¼‰
 * @returns é æ¸¬çµæœæ•¸çµ„ï¼ŒæŒ‰ç½®ä¿¡åº¦é™åºæ’åˆ—
 */
export async function classifyImage(
  imagePath: string,
  topK: number = 3
): Promise<Array<{ label: string; confidence: number }>> {
  try {
    // è¼‰å…¥æ¨¡å‹å’Œé¡åˆ¥åç¨±
    const model = await loadClassifierModel();
    const classNames = loadClassNames();

    // è¼‰å…¥ä¸¦é è™•ç†åœ–ç‰‡
    const imageTensor = await loadImageAsTensor(imagePath, IMAGE_SIZE);

    // é€²è¡Œé æ¸¬
    const predictions = model.predict(imageTensor) as tf.Tensor;

    // ç²å–é æ¸¬çµæœï¼ˆsoftmax è¼¸å‡ºï¼Œå·²ç¶“æ˜¯æ¦‚ç‡åˆ†ä½ˆï¼‰
    const predictionArray = await predictions.array();
    const probabilities = (predictionArray as number[][])[0];

    // æ¸…ç† tensor
    imageTensor.dispose();
    predictions.dispose();

    // ç”Ÿæˆçµæœæ•¸çµ„
    const results = probabilities
      .map((prob, index) => ({
        label: classNames[index],
        confidence: prob,
      }))
      .sort((a, b) => b.confidence - a.confidence)
      .slice(0, topK);

    return results;
  } catch (error) {
    console.error(`åˆ†é¡åœ–ç‰‡å¤±æ•—: ${imagePath}`, error);
    throw error;
  }
}

/**
 * æ‰¹é‡åˆ†é¡åœ–ç‰‡
 * @param imagePaths åœ–ç‰‡è·¯å¾‘æ•¸çµ„
 * @param topK è¿”å›å‰ K å€‹é æ¸¬çµæœï¼ˆé»˜èª 1ï¼‰
 * @param batchSize æ‰¹æ¬¡å¤§å°ï¼ˆé»˜èª 8ï¼‰
 * @returns é æ¸¬çµæœæ•¸çµ„ï¼ˆèˆ‡è¼¸å…¥é †åºå°æ‡‰ï¼‰
 */
export async function classifyImagesBatch(
  imagePaths: string[],
  topK: number = 1,
  batchSize: number = 8
): Promise<Array<{ path: string; predictions: Array<{ label: string; confidence: number }>; error?: string }>> {
  const results: Array<{ path: string; predictions: Array<{ label: string; confidence: number }>; error?: string }> = [];

  // åˆ†æ‰¹è™•ç†
  for (let i = 0; i < imagePaths.length; i += batchSize) {
    const batch = imagePaths.slice(i, i + batchSize);
    
    // ä¸¦è¡Œè™•ç†æ‰¹æ¬¡ä¸­çš„åœ–ç‰‡
    const batchResults = await Promise.all(
      batch.map(async (imagePath) => {
        try {
          const predictions = await classifyImage(imagePath, topK);
          return { path: imagePath, predictions };
        } catch (error) {
          return {
            path: imagePath,
            predictions: [],
            error: error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤',
          };
        }
      })
    );

    results.push(...batchResults);

    // é€²åº¦æç¤º
    if (i + batchSize < imagePaths.length) {
      process.stdout.write(`\r   å·²è™•ç†: ${Math.min(i + batchSize, imagePaths.length)}/${imagePaths.length}`);
    }
  }

  console.log(`\nâœ… æ‰¹é‡åˆ†é¡å®Œæˆ: ${results.length} å¼µåœ–ç‰‡`);

  return results;
}

/**
 * æ¸…é™¤ç·©å­˜çš„æ¨¡å‹ï¼ˆç”¨æ–¼é‡æ–°è¼‰å…¥ï¼‰
 */
export function clearModelCache(): void {
  if (cachedModel) {
    cachedModel.dispose();
    cachedModel = null;
  }
  cachedClassNames = null;
  console.log('ğŸ—‘ï¸  å·²æ¸…é™¤æ¨¡å‹ç·©å­˜');
}

/**
 * æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
 */
export function isModelAvailable(): boolean {
  const modelJsonPath = path.join(classifierModelDir, 'model.json');
  const weightsManifestPath = path.join(classifierModelDir, 'weights-manifest.json');
  const classNamesPath = path.join(classifierModelDir, 'classNames.json');

  return (
    fs.existsSync(modelJsonPath) &&
    fs.existsSync(weightsManifestPath) &&
    fs.existsSync(classNamesPath)
  );
}

/**
 * ç²å–æ¨¡å‹ä¿¡æ¯
 */
export function getModelInfo(): {
  available: boolean;
  modelPath: string;
  classNamesPath: string;
  numClasses?: number;
  classNames?: string[];
} {
  const modelJsonPath = path.join(classifierModelDir, 'model.json');
  const classNamesPath = path.join(classifierModelDir, 'classNames.json');
  const available = isModelAvailable();

  let numClasses: number | undefined;
  let classNames: string[] | undefined;

  if (available) {
    try {
      const loadedClassNames = JSON.parse(fs.readFileSync(classNamesPath, 'utf-8'));
      if (Array.isArray(loadedClassNames)) {
        classNames = loadedClassNames;
        numClasses = classNames.length;
      }
    } catch (error) {
      // å¿½ç•¥éŒ¯èª¤
    }
  }

  return {
    available,
    modelPath: classifierModelDir,
    classNamesPath,
    numClasses,
    classNames,
  };
}
